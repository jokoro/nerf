{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from PIL import Image\n",
    "from typing import *\n",
    "\n",
    "import cv2\n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "import wandb\n",
    "\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images and declare constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'bedroom'\n",
    "name = 'forest1'\n",
    "name = 'forest2'\n",
    "name = 'sidewalk'\n",
    "name = 'study'\n",
    "name = 'kitchen'\n",
    "name = 'bottle'\n",
    "name = 'apples'\n",
    "name = 'sourcream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE_PATH = Path(f'colmap/{name}/sparse')\n",
    "IMG_PATH = Path(f'colmap/{name}/images')\n",
    "IMG_PATH.mkdir(exist_ok=True)\n",
    "SEED = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# cap = cv2.VideoCapture(f'videos/{name}.MOV')\n",
    "\n",
    "# frame_no = 0\n",
    "# every_n_frames = 10\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     if frame_no % every_n_frames == 0:\n",
    "#         target = imgpath.joinpath(f'{frame_no:06d}.jpg').as_posix()\n",
    "#         cv2.imwrite(target, frame)\n",
    "        \n",
    "#     frame_no += 1\n",
    "#     if not ret: break\n",
    "\n",
    "# cap.release()\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera class and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <__main__.Camera at 0x128050fb0>}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize camera objects (for now only one)\n",
    "\n",
    "class Camera:\n",
    "    def __init__(self, camera_id, model, width, height, params):\n",
    "        self.camera_id = camera_id\n",
    "        self.model = model\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.params = params\n",
    "        self.K = self._get_K()                  # size: 3 x 3\n",
    "        self.K_inv = self.K.inverse()           # size: 3 x 3\n",
    "        self.xy_pairs = self._get_xy_pairs()    # size: H*W, H*W\n",
    "        self.d_camera = self._get_d()           # size: H*W x 3\n",
    "    \n",
    "    def _get_K(self):\n",
    "        if self.model == 'PINHOLE':\n",
    "            fx, fy, cx, cy = self.params\n",
    "            K = torch.tensor([\n",
    "                [fx, 0,  cx],\n",
    "                [0,  fy, cy],\n",
    "                [0,  0,  1 ],\n",
    "            ])\n",
    "        elif self.model == 'SIMPLE_PINHOLE':\n",
    "            f, cx, cy = self.params\n",
    "            K = torch.tensor([\n",
    "                [f, 0, cx],\n",
    "                [0, f, cy],\n",
    "                [0, 0, 1 ],\n",
    "            ])\n",
    "        return K\n",
    "    \n",
    "    def _get_xy_pairs(self):\n",
    "        y, x = torch.unravel_index(\n",
    "            torch.arange(self.height * self.width), \n",
    "            (self.height, self.width)\n",
    "        )\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def _get_d(self):\n",
    "        x, y = self.xy_pairs\n",
    "        x = x.unsqueeze(0)                  # size: 1 x HW\n",
    "        y = y.unsqueeze(0)                  # size: 1 x HW\n",
    "                                            \n",
    "        x_y_1 = torch.cat(                  \n",
    "            (x, y, torch.ones_like(x))      \n",
    "        ).float()                           # size: 3 x HW\n",
    "        d = self.K_inv @ x_y_1              # size: (3 x 3) @ (3 x HW) = 3 x HW\n",
    "        d = (d / d.norm(dim=0)).T           # size: HW x 3\n",
    "                                                    \n",
    "        return d                                               \n",
    "\n",
    "\n",
    "def get_cameras():\n",
    "    camera_file=f'colmap/{name}/sparse/cameras.txt'\n",
    "\n",
    "    with open(camera_file) as file:\n",
    "        camera_lines = file.readlines()[3:]\n",
    "    \n",
    "    cameras = {}\n",
    "    for camera_line in camera_lines:\n",
    "        (camera_id, model, width, height, *params) = camera_line.strip('\\n').split()\n",
    "        camera_id, width, height = (int(s) for s in (camera_id, width, height))\n",
    "        params = [float(s) for s in params]\n",
    "        camera = Camera(camera_id, model, width, height, params)\n",
    "        cameras[camera_id] = camera\n",
    "        \n",
    "    return cameras\n",
    "\n",
    "cameras: Dict[int, Camera] = get_cameras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImagePose class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImagePose:\n",
    "    def __init__(self, image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name):\n",
    "        self.image_id = image_id\n",
    "        self.r = self._quaternions_to_matrix(qw, qx, qy, qz)\n",
    "        self.t = torch.tensor([tx, ty, tz])\n",
    "        self.camera_id = camera_id\n",
    "        self.name = name\n",
    "        self.image = self._imgfile_to_tensor()\n",
    "    \n",
    "    def _quaternions_to_matrix(self, qw, qx, qy, qz):\n",
    "        # R = Rotation.from_quat([qw, qx, qy, qz])\n",
    "        # R = torch.tensor(R.as_matrix())\n",
    "        \n",
    "        R = torch.tensor([\n",
    "            [1 - 2 * (qy * qy + qz * qz),   2 * (qx * qy - qz * qw),        2 * (qx * qz + qy * qw)],\n",
    "            [2 * (qx * qy + qz * qw),       1 - 2 * (qx * qx + qz * qz),    2 * (qy * qz - qx * qw)],\n",
    "            [2 * (qx * qz - qy * qw),       2 * (qy * qz + qx * qw),        1 - 2 * (qx * qx + qy * qy)],\n",
    "        ])\n",
    "        return R\n",
    "    \n",
    "    def _imgfile_to_tensor(self):\n",
    "        path = IMG_PATH.joinpath(self.name).as_posix()\n",
    "        image = Image.open(path)\n",
    "        image_tensor = F.to_tensor(image)\n",
    "        return image_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'run': 'initial-run',\n",
    "    'batch_size': 1024,\n",
    "    'learning_rate': 0.02,\n",
    "    'num_epochs': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40404.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonoko\u001b[0m (\u001b[33m11785_group51\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='nerf',\n",
    "    name=config['run'],\n",
    "    config=config,\n",
    "    id=None, # id to resume\n",
    "    resume=False, # if want to resume\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonoko\u001b[0m (\u001b[33m11785_group51\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "^C\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nerf-env/lib/python3.8/site-packages/wandb/sdk/wandb_manager.py\", line 176, in _teardown\n",
      "    result = self._service.join()\n",
      "  File \"/opt/anaconda3/envs/nerf-env/lib/python3.8/site-packages/wandb/sdk/service/service.py\", line 263, in join\n",
      "    ret = self._internal_proc.wait()\n",
      "  File \"/opt/anaconda3/envs/nerf-env/lib/python3.8/subprocess.py\", line 1083, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/anaconda3/envs/nerf-env/lib/python3.8/subprocess.py\", line 1822, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/anaconda3/envs/nerf-env/lib/python3.8/subprocess.py\", line 1780, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# !wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SPARSE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCollects 3D points from points_path and pairs it with various viewing \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mdirections and cooresponding colors from those directions using images_path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m            c is tensor of RGB value            size: 3 or B x 3\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTrainDataset\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPARSE_PATH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_pixels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mTrainDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainDataset\u001b[39;00m(Dataset):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images_path\u001b[38;5;241m=\u001b[39m\u001b[43mSPARSE_PATH\u001b[49m\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mas_posix()):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pixels \u001b[38;5;241m=\u001b[39m cameras[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m*\u001b[39m cameras[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwidth\n\u001b[1;32m     13\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m cameras[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mxy_pairs                          \u001b[38;5;66;03m# size: HW\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SPARSE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Collects 3D points from points_path and pairs it with various viewing \n",
    "directions and cooresponding colors from those directions using images_path.\n",
    "Output format: (x, d), c\n",
    "    where   x is tensor of 3D location          size: 3 or B x 3\n",
    "            d is tensor of viewing direction    size: 3 or B x 3\n",
    "            c is tensor of RGB value            size: 3 or B x 3\n",
    "'''\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, images_path=SPARSE_PATH.joinpath('images.txt').as_posix()):\n",
    "        self.num_pixels = cameras[1].height * cameras[1].width\n",
    "        \n",
    "        x, y = cameras[1].xy_pairs                          # size: HW\n",
    "        d_camera = cameras[1].d_camera                      # size: HW x 3 \n",
    "        f, cx, cy = cameras[1].params # cx, cy = W/2, H/2\n",
    "\n",
    "        # values for NDC projection\n",
    "        # paper uses [-cx, -cy, f] b/c they use (y up, z into camera)\n",
    "        # but we use [cx, cy, f] b/c we use (y down, z out of camera) \n",
    "        # b/c of colmap, but also I prefer colmap's way and would use it again\n",
    "        scalar = torch.tensor([f/cx, f/cy, 1]) #  = f / [cx, cy, f]\n",
    "        two_f = 2 * f\n",
    "        \n",
    "        with open(images_path) as file:\n",
    "            image_lines = file.readlines()[4::2]\n",
    "\n",
    "        o_list, d_list, c_list = [], [], []\n",
    "        for image_line in image_lines:\n",
    "            pose = self._get_image_pose(image_line)\n",
    "            \n",
    "            # # for pose-dependent camera\n",
    "            # x, y = cameras[pose.camera_id].xy_pairs       # size: HW\n",
    "            # d_camera = cameras[pose.camera_id].d_camera   # size: HW x 3 \n",
    "            # f, cx, cy = cameras[pose.camera_id].params \n",
    "            # # cx, cy = W/2, H/2\n",
    "            \n",
    "            # find origin of camera and directions from origin \n",
    "            # to pixels in world coordinates and get c (colors)\n",
    "            r, t = pose.r, pose.t.unsqueeze(0)              # size: 3x3, 1x3\n",
    "            o = -t @ r          # -r.T @ t                  # size: 1 x 3\n",
    "            d = d_camera @ r    #  r.T @ d_camera           # size: HW x 3\n",
    "            c = pose.image[:, y, x].T                       # size: HW x 3\n",
    "                                                            \n",
    "            # NcccccDC projection                                \n",
    "            #   links to understand NDC better:\n",
    "            #   https://www.youtube.com/watch?v=U0_ONQQ5ZNM\n",
    "            #   https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis\n",
    "            o = o / o[:,2:3]                                \n",
    "            d = d / d[:,2:3]                                \n",
    "                                                            \n",
    "            o[:,2] += two_f                                 \n",
    "            d -= o                                          \n",
    "                                                            \n",
    "            o *= scalar                                     \n",
    "            d *= scalar                                     \n",
    "\n",
    "            # resize to use broadcasting for stratified \n",
    "            # sampling at training/inference stage.\n",
    "            # doing it here is more efficient\n",
    "            o.unsqueeze_(1)                                 # size: 1x1x3\n",
    "            d.unsqueeze_(1)                                 # size: HWx1x3\n",
    "                                                            \n",
    "            # add to dataset                                \n",
    "            o_list.append(o)                                \n",
    "            d_list.append(d)                                \n",
    "            c_list.append(c)                                \n",
    "                                                            # A = num_images\n",
    "        self.o_tensor = torch.cat(o_list)                   # size: A x 1 x 3\n",
    "        self.d_tensor = torch.cat(d_list)                   # size: HWA x 1 x 3\n",
    "        self.c_tensor = torch.cat(c_list)                   # size: HWA x 3\n",
    "                    \n",
    "    # converts line in image.txt to ImagePose object\n",
    "    def _get_image_pose(self, image_line):\n",
    "        (image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name\n",
    "        ) = image_line.strip('\\n').split()\n",
    "        \n",
    "        image_id, camera_id = int(image_id), int(camera_id)\n",
    "        (qw, qx, qy, qz, tx, ty, tz, \n",
    "        ) = (float(s) for s in (qw, qx, qy, qz, tx, ty, tz))\n",
    "        \n",
    "        pose = ImagePose(image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name)\n",
    "        \n",
    "        return pose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.d_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        o = self.o_tensor[idx//self.num_pixels]             # size: 1 x 3\n",
    "        d = self.d_tensor[idx]                              # size: 1 x 3\n",
    "        c = self.c_tensor[idx]                              # size: 3\n",
    "        return o, d, c\n",
    "    \n",
    "trainset = TrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): # tested\n",
    "    def __init__(self, in_feat, out_feat, activation=nn.ReLU()):\n",
    "        super().__init__()     \n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(in_feat, out_feat),\n",
    "            activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module): # tested\n",
    "\n",
    "    def __init__(self, L):\n",
    "        super().__init__()\n",
    "\n",
    "        self.omega = 2**torch.arange(0, L, 1/2).int()*torch.pi\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x):                               # size: BxNx3\n",
    "        gamma = x.unsqueeze(-1) * self.omega            # size: (BxNx3x1) * (2L) -> BxNx3x2L\n",
    "        gamma[...,::2] = torch.sin(gamma[...,::2])\n",
    "        gamma[...,1::2] = torch.cos(gamma[...,1::2])\n",
    "        gamma = gamma.flatten(-2)                       # size: BxNx3x2L = BxNx6L\n",
    "        return gamma \n",
    "\n",
    "class Nerf(nn.Module): # tested\n",
    "    def __init__(self, Lx, Ld):\n",
    "        super().__init__()\n",
    "        self.pos_enc_x = PositionalEncoding(Lx)\n",
    "        self.pos_enc_d = PositionalEncoding(Ld)\n",
    "        \n",
    "        self.bfr_x_res = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MLP(Lx*6,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "        )\n",
    "        self.bfr_d_in = nn.Sequential(\n",
    "            MLP(256+Lx*6,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,257,nn.Identity()),\n",
    "        )\n",
    "        self.aft_d_in = nn.Sequential(\n",
    "            MLP(256+Ld*6,128),\n",
    "            MLP(128,3,nn.Sigmoid()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, d):                        #   size: BxNx3, BxNx3\n",
    "        gamma_x = self.pos_enc_x(x)                 #   size: BxNx6Lx\n",
    "        out = self.bfr_x_res(gamma_x)               #   size: BxNx256\n",
    "        out = torch.cat((out, gamma_x), -1)         #   size: BxNx(256+6Lx)\n",
    "        out = self.bfr_d_in(out)                    #   size: BxNx257\n",
    "        sig, out = out[:,0:1], out[:,1:]            #   size: BxNx1, BxNx256\n",
    "        gamma_d = self.pos_enc_d(d)                 #   size: BxNx6Ld\n",
    "        out = torch.cat((out, gamma_d), -1)         #   size: BxNx(256+6Ld)\n",
    "        rgb = self.aft_d_in(out)                    #   size: BxNx3\n",
    "        \n",
    "        sig += torch.randn_like(sig)                \n",
    "        # paper: ^ is helpful for real scenes               \n",
    "        sig = sig.relu()                                 \n",
    "\n",
    "        return rgb, sig                             #   size: BxNx3, BxNx1\n",
    "\n",
    "model = Nerf(10, 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_t\u001b[39m(i, tn, tf, Nc, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTensor\u001b[49m:\n\u001b[1;32m      2\u001b[0m     u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(b, Nc, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(Nc, \u001b[38;5;241m1\u001b[39m)                \u001b[38;5;66;03m# size: if b: BxNcx1 else: Ncx1\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     i_random \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m u                                                    \u001b[38;5;66;03m# size: if b: BxNcx1 else: Ncx1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "def color_weights_and_x(\n",
    "    tn: float, \n",
    "    tf: float, \n",
    "    N: int, \n",
    "    i_random: Tensor, \n",
    "    o: Tensor, \n",
    "    d: Tensor, \n",
    "    model: Nerf, \n",
    "    x_cat: Tensor = None\n",
    "):\n",
    "    t_i = tn + i_random * (tf - tn) / N                                 # size: BxNx1 \n",
    "    \n",
    "    x = o + t_i * d # in NDC coords :)                                  # size: (Bx1x3) + (N1x1) * (Bx1x3) -> BxN1x3\n",
    "    if x_cat: x = torch.cat((x, x_cat), -2)                             # size: cat((BxN1x3),(BxN2x3), -2) -> Bx(N1+N2=N)x3\n",
    "    d = d.tile((1,x.shape[-2],1))                                       # size: BxNx3\n",
    "    \n",
    "    c_i, sigma_i: Tuple[Tensor, Tensor] = model(x, d)                                          # type: ignore # size: input: BxNx3, Bx3 | output: BxNx3, BxNx1\n",
    "\n",
    "    delta_i = t_i.roll(-1, -2) - t_i                                    \n",
    "    delta_i[-1] = 0                                                     # size: N x 1\n",
    "\n",
    "    neg_dlt_sig_i: Tensor = -delta_i * sigma_i  # [a, b, ..., 0]        # size: (N x 1) * (B x N x 1) -> B x N x 1\n",
    "    neg_dlt_sig_im1 = neg_dlt_sig_i.roll(1, -2) # [0, a, b, ...]        # size: B x N x 1\n",
    "    T_i = neg_dlt_sig_im1.cumsum(-2)            # [0, A, B, ...]        # size: B x N x 1\n",
    "\n",
    "    w = T_i * (1 - torch.exp(neg_dlt_sig_i))                            # size: (BxNx1) * (BxNx1) -> BxNx1\n",
    "    c = (w * c_i).sum(-2)                                               # size: ((BxNx1) * (BxNx3) -> BxNx3).sum(-2) -> Bx3\n",
    "    \n",
    "    return c, w, x\n",
    "\n",
    "def get_i_fine(\n",
    "    w: Tensor, \n",
    "    b: int, \n",
    "    Nf: int, \n",
    "    i: Tensor\n",
    "):\n",
    "    w_hat = w / w.sum(-2, True)                                     # size: (BxNcx1) / ((BxNcx1).sum(-2, True) -> Bx1x1) -> BxNcx1\n",
    "    w_hat_cum = w_hat.cumsum(-2)                                    # size: BxNcx1\n",
    "    \n",
    "    u = torch.rand(b, 1, Nf)                                        # size: Bx1xNf\n",
    "    idx = (u > w_hat_cum).sum(-2) # 0≤u≤w0: [F,F,F,...].sum: idx=0  # size: ((Bx1xNf) > (BxNcx1) -> BxNcxNf).sum(-2) -> BxNf\n",
    "    \n",
    "    w_hat_cum = w_hat_cum.roll(1, -2)\n",
    "    w_hat_cum[:,0] = 0\n",
    "    i_fine = (u - w_hat_cum) / w_hat + i # inv transf sample funcs  # size: ((Bx1xNf) - (BxNcx1) -> BxNcxNf) / (BxNcx1) + (Ncx1) -> BxNcxNf\n",
    "    \n",
    "    q = idx.flatten()                                               # size: BNf\n",
    "    p, r = torch.unravel_index(torch.arange(b*Nf), (b, Nf))         # size: BNf, BNf\n",
    "    i_fine = i_fine[p, q, r].view(b, Nf, 1) # pick correct function # size: BxNfx1\n",
    "    \n",
    "    return i_fine\n",
    "\n",
    "\n",
    "# def train(model, trainloader):\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# t: NDC coords for ray parameter\n",
    "# Nc: num bins along ray at first\n",
    "# Nf: num new samples based on Nc dist.\n",
    "tn, tf, Nc, Nf = 0, 1, 20, 40 \n",
    "i = torch.arange(Nc).unsqueeze(-1)                                              # size: Ncx1\n",
    "for epoch in range(config['num_epochs']):   \n",
    "    for o, d, c in trainloader: # o,d in NDC coords                             # size: Bx1x3, Bx1x3, Bx3        \n",
    "        optimizer.zero_grad()   \n",
    "        b = o.shape[0] # might change in last batch \n",
    "\n",
    "        # get coarse colors \n",
    "        i_coarse = i + torch.rand(b, Nc, 1)                                     # size: (Ncx1) + (BxNcx1) -> BxNcx1\n",
    "        c_coarse, w, x = color_weights_and_x(tn, tf, Nc, i_coarse, o, d, model) # Bx3, BxNcx1\n",
    "\n",
    "        # get fine colors   \n",
    "        i_fine = get_i_fine(w, b, Nf, i)    \n",
    "        c_fine, _, _ = color_weights_and_x(tn, tf, Nf, i_fine, o, d, model, x)  # Bx3, BxNcx1\n",
    "        \n",
    "        loss: Tensor = criterion(c_coarse, c) + criterion(c_fine, c)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  1, 10,  3],\n",
       "        [16, 13, 18, 19]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "x,y,z = 2,3,4\n",
    "a = torch.arange(x*y*z).view(x,y,z)\n",
    "# tensor([[[ 0,  1,  2,  3],\n",
    "#          [ 4,  5,  6,  7],\n",
    "#          [ 8,  9, 10, 11]],\n",
    "\n",
    "#         [[12, 13, 14, 15],\n",
    "#          [16, 17, 18, 19],\n",
    "#          [20, 21, 22, 23]]])\n",
    "b = torch.randint(y, size=(x,z))\n",
    "# tensor([[[2, 0, 2, 0]],\n",
    "\n",
    "#         [[1, 0, 1, 1]]])\n",
    "u, w = torch.unravel_index(\n",
    "    torch.arange(x*z), \n",
    "    (x, z)\n",
    ")\n",
    "# u = tensor([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "# w = tensor([0, 1, 2, 3, 0, 1, 2, 3])\n",
    "# v = [2, 0, 2, 0, 1, 0, 1, 1]\n",
    "v = b.flatten()\n",
    "a[u,v,w].view(x, z) # x,z\n",
    "# tensor([[ 8,  1, 10,  3],\n",
    "#         [16, 13, 18, 19]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model.pth\"\n",
    "model.save(model_path)\n",
    "wandb.save(model_path)\n",
    "print(\"Model saved to Weights and Biases!\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Run stopped\")\n",
    "\n",
    "# # restore model\n",
    "# best_model = wandb.restore(\n",
    "#     model_path, \n",
    "#     run_path=f'jay-okoro/nerf/{config['run']}'\n",
    "# )\n",
    "# model.load_weights(best_model.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
