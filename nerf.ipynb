{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle to False to skip long pre-processes\n",
    "restart = False\n",
    "restart = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from PIL import Image\n",
    "from typing import *\n",
    "\n",
    "import cv2\n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import einops \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images and declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'bedroom'\n",
    "name = 'forest1'\n",
    "name = 'forest2'\n",
    "name = 'sidewalk'\n",
    "name = 'study'\n",
    "name = 'kitchen'\n",
    "name = 'bottle'\n",
    "name = 'apples'\n",
    "name = 'sourcream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE_PATH = Path(f'colmap/{name}/sparse')\n",
    "IMG_PATH = Path(f'colmap/{name}/images')\n",
    "IMG_PATH.mkdir(exist_ok=True)\n",
    "SEED = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(f'videos/{name}.MOV')\n",
    "\n",
    "# frame_no = 0\n",
    "# every_n_frames = 10\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     if frame_no % every_n_frames == 0:\n",
    "#         target = imgpath.joinpath(f'{frame_no:06d}.jpg').as_posix()\n",
    "#         cv2.imwrite(target, frame)\n",
    "        \n",
    "#     frame_no += 1\n",
    "#     if not ret: break\n",
    "\n",
    "# cap.release()\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera class and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize camera objects (for now only one)\n",
    "\n",
    "class Camera:\n",
    "    def __init__(self, camera_id, model, width, height, params):\n",
    "        self.camera_id = camera_id\n",
    "        self.model = model\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.params = params\n",
    "        self.K = self._get_K()                  # size: 3 x 3\n",
    "        self.K_inv = self.K.inverse()           # size: 3 x 3\n",
    "        self.xy_pairs = self._get_xy_pairs()    # size: H*W, H*W\n",
    "        self.d_camera = self._get_d()           # size: H*W x 3\n",
    "    \n",
    "    def _get_K(self):\n",
    "        if self.model == 'PINHOLE':\n",
    "            fx, fy, cx, cy = self.params\n",
    "            K = torch.tensor([\n",
    "                [fx, 0,  cx],\n",
    "                [0,  fy, cy],\n",
    "                [0,  0,  1 ],\n",
    "            ])\n",
    "        elif self.model == 'SIMPLE_PINHOLE':\n",
    "            f, cx, cy = self.params\n",
    "            K = torch.tensor([\n",
    "                [f, 0, cx],\n",
    "                [0, f, cy],\n",
    "                [0, 0, 1 ],\n",
    "            ])\n",
    "        return K\n",
    "    \n",
    "    def _get_xy_pairs(self):\n",
    "        y, x = torch.unravel_index(\n",
    "            torch.arange(self.height * self.width), \n",
    "            (self.height, self.width)\n",
    "        )\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def _get_d(self):\n",
    "        x, y = self.xy_pairs\n",
    "        x = x.unsqueeze(0)                  # size: 1 x HW\n",
    "        y = y.unsqueeze(0)                  # size: 1 x HW\n",
    "                                            \n",
    "        x_y_1 = torch.cat(                  \n",
    "            (x, y, torch.ones_like(x))      \n",
    "        ).float()                           # size: 3 x HW\n",
    "        d = self.K_inv @ x_y_1              # size: (3 x 3) @ (3 x HW) = 3 x HW\n",
    "        d = (d / d.norm(dim=0)).T           # size: HW x 3\n",
    "                                                    \n",
    "        return d                                               \n",
    "\n",
    "\n",
    "def get_cameras():\n",
    "    camera_file=f'colmap/{name}/sparse/cameras.txt'\n",
    "\n",
    "    with open(camera_file) as file:\n",
    "        camera_lines = file.readlines()[3:]\n",
    "    \n",
    "    cameras = {}\n",
    "    for camera_line in camera_lines:\n",
    "        (camera_id, model, width, height, *params) = camera_line.strip('\\n').split()\n",
    "        camera_id, width, height = (int(s) for s in (camera_id, width, height))\n",
    "        params = [float(s) for s in params]\n",
    "        camera = Camera(camera_id, model, width, height, params)\n",
    "        cameras[camera_id] = camera\n",
    "        \n",
    "    return cameras\n",
    "\n",
    "cameras: Dict[int, Camera] = get_cameras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImagePose class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImagePose:\n",
    "    def __init__(self, image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name):\n",
    "        self.image_id = image_id\n",
    "        self.r = self._quaternions_to_matrix(qw, qx, qy, qz)\n",
    "        self.t = torch.tensor([tx, ty, tz])\n",
    "        self.camera_id = camera_id\n",
    "        self.name = name\n",
    "        self.image = self._imgfile_to_tensor()\n",
    "    \n",
    "    def _quaternions_to_matrix(self, qw, qx, qy, qz):\n",
    "        # R = Rotation.from_quat([qw, qx, qy, qz])\n",
    "        # R = torch.tensor(R.as_matrix())\n",
    "        \n",
    "        R = torch.tensor([\n",
    "            [1 - 2 * (qy * qy + qz * qz),   2 * (qx * qy - qz * qw),        2 * (qx * qz + qy * qw)],\n",
    "            [2 * (qx * qy + qz * qw),       1 - 2 * (qx * qx + qz * qz),    2 * (qy * qz - qx * qw)],\n",
    "            [2 * (qx * qz - qy * qw),       2 * (qy * qz + qx * qw),        1 - 2 * (qx * qx + qy * qy)],\n",
    "        ])\n",
    "        return R\n",
    "    \n",
    "    def _imgfile_to_tensor(self):\n",
    "        path = IMG_PATH.joinpath(self.name).as_posix()\n",
    "        image = Image.open(path)\n",
    "        image_tensor = F.to_tensor(image)\n",
    "        return image_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'initial-run',\n",
    "    'batch_size': 4096,\n",
    "    'initial_lr': 5e-4,\n",
    "    'final_lr': 5e-5,\n",
    "    'num_iter': int(100e3),\n",
    "    \n",
    "    # these two probably shouldn't change. \n",
    "    # consider removing from config\n",
    "    'tn': 0,\n",
    "    'tf': 1,\n",
    "    \n",
    "    'Nc': 64,\n",
    "    'Nf': 128,\n",
    "    'Lx': 10, \n",
    "    'Ld': 4, \n",
    "}\n",
    "B, TN, TF, NC, NF = config['batch_size'], config['tn'], config['tf'], config['Nc'], config['Nf'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"nerf.ipynb\"\n",
    "# in terminal: \n",
    "# >> wandb login --relogin\n",
    "# paste API key from your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjay-okoro\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jasonokoro/Desktop/proyectos/nerf/wandb/run-20240427_220648-514fbzzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jay-okoro/nerf/runs/514fbzzo' target=\"_blank\">initial-run</a></strong> to <a href='https://wandb.ai/jay-okoro/nerf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jay-okoro/nerf' target=\"_blank\">https://wandb.ai/jay-okoro/nerf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jay-okoro/nerf/runs/514fbzzo' target=\"_blank\">https://wandb.ai/jay-okoro/nerf/runs/514fbzzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='nerf',\n",
    "    name=config['name'],\n",
    "    reinit=True,\n",
    "    config=config,\n",
    "    # id=None, # id of run to resume\n",
    "    # resume='must', # if want to resume, comment reinit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'514fbzzo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    restart\n",
    "    # or True\n",
    "):\n",
    "    '''\n",
    "    Collects 3D points from points_path and pairs it with various viewing \n",
    "    directions and cooresponding colors from those directions using images_path.\n",
    "    Output format: (x, d), c\n",
    "        where   x is tensor of 3D location          size: 3 or B x 3\n",
    "                d is tensor of viewing direction    size: 3 or B x 3\n",
    "                c is tensor of RGB value            size: 3 or B x 3\n",
    "    '''\n",
    "    class TrainDataset(Dataset):\n",
    "        def __init__(self, images_path=SPARSE_PATH.joinpath('images.txt').as_posix()):\n",
    "            self.num_pixels = cameras[1].height * cameras[1].width\n",
    "            \n",
    "            x, y = cameras[1].xy_pairs                          # size: HW\n",
    "            d_camera = cameras[1].d_camera                      # size: HW x 3 \n",
    "            f, cx, cy = cameras[1].params # cx, cy = W/2, H/2\n",
    "\n",
    "            # values for NDC projection\n",
    "            # paper uses [-cx, -cy, f] b/c they use (y up, z into camera)\n",
    "            # but we use [cx, cy, f] b/c we use (y down, z out of camera) \n",
    "            # b/c of colmap, but also I prefer colmap's way and would use it again\n",
    "            scalar = torch.tensor([f/cx, f/cy, 1]) #  = f / [cx, cy, f]\n",
    "            two_f = 2 * f\n",
    "            \n",
    "            with open(images_path) as file:\n",
    "                image_lines = file.readlines()[4::2]\n",
    "\n",
    "            o_list, d_list, c_list = [], [], []\n",
    "            for image_line in image_lines:\n",
    "                pose = self._get_image_pose(image_line)\n",
    "                \n",
    "                # # for pose-dependent camera\n",
    "                # x, y = cameras[pose.camera_id].xy_pairs       # size: HW\n",
    "                # d_camera = cameras[pose.camera_id].d_camera   # size: HW x 3 \n",
    "                # f, cx, cy = cameras[pose.camera_id].params \n",
    "                # # cx, cy = W/2, H/2\n",
    "                \n",
    "                # find origin of camera and directions from origin \n",
    "                # to pixels in world coordinates and get c (colors)\n",
    "                r, t = pose.r, pose.t.unsqueeze(0)              # size: 3x3, 1x3\n",
    "                o = -t @ r          # -r.T @ t                  # size: 1 x 3\n",
    "                d = d_camera @ r    #  r.T @ d_camera           # size: HW x 3\n",
    "                c = pose.image[:, y, x].T                       # size: HW x 3\n",
    "                                                                \n",
    "                # NDC projection                                \n",
    "                #   links to understand NDC better:\n",
    "                #   https://www.youtube.com/watch?v=U0_ONQQ5ZNM\n",
    "                #   https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis\n",
    "                o = o / o[:,2:3]                                \n",
    "                d = d / d[:,2:3]                                \n",
    "                                                                \n",
    "                o[:,2] += two_f                                 \n",
    "                d -= o                                          \n",
    "                                                                \n",
    "                o *= scalar                                     \n",
    "                d *= scalar                                     \n",
    "\n",
    "                # resize to use broadcasting for stratified \n",
    "                # sampling at training/inference stage.\n",
    "                # doing it here is more efficient\n",
    "                o = o.unsqueeze(1)                              # size: 1x1x3\n",
    "                d = d.unsqueeze(1)                              # size: HWx1x3\n",
    "                                                                \n",
    "                # add to dataset                                \n",
    "                o_list.append(o)                                \n",
    "                d_list.append(d)                                \n",
    "                c_list.append(c)                                \n",
    "                                                                # A = num_images\n",
    "            self.o_tensor = torch.cat(o_list)                   # size: A x 1 x 3\n",
    "            self.d_tensor = torch.cat(d_list)                   # size: HWA x 1 x 3\n",
    "            self.c_tensor = torch.cat(c_list)                   # size: HWA x 3\n",
    "                        \n",
    "        # converts line in image.txt to ImagePose object\n",
    "        def _get_image_pose(self, image_line):\n",
    "            (image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name\n",
    "            ) = image_line.strip('\\n').split()\n",
    "            \n",
    "            image_id, camera_id = int(image_id), int(camera_id)\n",
    "            (qw, qx, qy, qz, tx, ty, tz, \n",
    "            ) = (float(s) for s in (qw, qx, qy, qz, tx, ty, tz))\n",
    "            \n",
    "            pose = ImagePose(image_id, qw, qx, qy, qz, tx, ty, tz, camera_id, name)\n",
    "            \n",
    "            return pose\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.d_tensor)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            o = self.o_tensor[idx//self.num_pixels]             # size: 1 x 3\n",
    "            d = self.d_tensor[idx]                              # size: 1 x 3\n",
    "            c = self.c_tensor[idx]                              # size: 3\n",
    "            return o, d, c\n",
    "        \n",
    "    trainset = TrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "num_epochs = config['num_iter'] * config['batch_size'] // len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): # tested\n",
    "    def __init__(self, in_feat, out_feat, activation=nn.ReLU()):\n",
    "        super().__init__()     \n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(in_feat, out_feat),\n",
    "            activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module): # tested\n",
    "\n",
    "    def __init__(self, L):\n",
    "        super().__init__()\n",
    "\n",
    "        self.L = L\n",
    "        self.omega = 2**torch.arange(0, L, 1/2).int() * torch.pi\n",
    "\n",
    "    def forward(self, x):                                       # size: BxNx3\n",
    "        gamma = x.unsqueeze(-1) * self.omega                    # size: (BxNx3x1) * (2L) -> BxNx3x2L\n",
    "        gamma = einops.rearrange([torch.sin(gamma[...,::2]),\n",
    "                                  torch.cos(gamma[...,1::2])], \n",
    "                                 't b n h w -> b n h (w t)')\n",
    "        gamma = gamma.flatten(-2)                               # size: BxNx3x2L = BxNx6L\n",
    "        return gamma \n",
    "\n",
    "class Nerf(nn.Module): # tested\n",
    "    def __init__(self, Lx, Ld):\n",
    "        super().__init__()\n",
    "        self.pos_enc_x = PositionalEncoding(Lx)\n",
    "        self.pos_enc_d = PositionalEncoding(Ld)\n",
    "        \n",
    "        self.bfr_x_res = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            MLP(Lx*6,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "        )\n",
    "        self.bfr_d_in = nn.Sequential(\n",
    "            MLP(256+Lx*6,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,256),\n",
    "            MLP(256,257,nn.Identity()),\n",
    "        )\n",
    "        self.aft_d_in = nn.Sequential(\n",
    "            MLP(256+Ld*6,128),\n",
    "            MLP(128,3,nn.Sigmoid()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, d):                        #   size: BxNx3, BxNx3\n",
    "        gamma_x = self.pos_enc_x(x)                 #   size: BxNx6Lx\n",
    "        out = self.bfr_x_res(gamma_x)               #   size: BxNx256\n",
    "        out = torch.cat((out, gamma_x), -1)         #   size: BxNx(256+6Lx)\n",
    "        out = self.bfr_d_in(out)                    #   size: BxNx257\n",
    "        sig, out = out[:,:,0:1], out[:,:,1:]        #   size: BxNx1, BxNx256\n",
    "        gamma_d = self.pos_enc_d(d)                 #   size: BxNx6Ld\n",
    "        out = torch.cat((out, gamma_d), -1)         #   size: BxNx(256+6Ld) torch.Size([1024, 63, 257]) torch.Size([1024, 64, 24])\n",
    "        rgb = self.aft_d_in(out)                    #   size: BxNx3\n",
    "        \n",
    "        sig += torch.randn_like(sig) # paper says this is helpful for real scenes               \n",
    "        sig = sig.relu()                                 \n",
    "\n",
    "        return rgb, sig                             #   size: BxNx3, BxNx1\n",
    "\n",
    "model = Nerf(\n",
    "    config['Lx'],\n",
    "    config['Ld']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config['initial_lr'], eps=1.e-7)\n",
    "gamma = - (np.log(config['final_lr']) - np.log(config['initial_lr'])) / num_epochs\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_i_mask = torch.ones(config['Nc'], 1, requires_grad=False)\n",
    "delta_i_mask_cat = torch.ones(config['Nc'] + config['Nf'], 1, requires_grad=False)\n",
    "delta_i_mask[-1] = 0\n",
    "delta_i_mask_cat[-1] = 0\n",
    "\n",
    "def color_weights_and_t_i(\n",
    "    tn: float, \n",
    "    tf: float, \n",
    "    N: int, \n",
    "    i_random: Tensor, \n",
    "    o: Tensor, \n",
    "    d: Tensor, \n",
    "    model: Nerf, \n",
    "    t_i_cat: Tensor = None\n",
    "):\n",
    "    t_i = tn + i_random * (tf - tn) / N                                 # size: BxN1x1 \n",
    "    if t_i_cat is not None: \n",
    "        # concat fine and coarse pts using t parameter\n",
    "        # t's must be in right order or integral will be wrong\n",
    "        t_i = torch.cat((t_i, t_i_cat), -2).sort(-2)[0]                 # size: cat((BxN1x1),(BxN2x1), -2) -> Bx(N1+N2=N)x1\n",
    "    \n",
    "    x = o + t_i * d # in NDC coords :)                                  # size: (Bx1x3) + (BxNx1) * (Bx1x3) -> BxNx3\n",
    "    d = d.tile((1, x.shape[-2], 1))                                     # size: BxNx3\n",
    "    \n",
    "    c_i, sigma_i = model(x, d)                                          # size: input: BxNx3, Bx3 | output: BxNx3, BxNx1\n",
    "    c_i: Tuple = c_i\n",
    "    sigma_i: Tuple = sigma_i\n",
    "\n",
    "    delta_i = (t_i.roll(-1, -2) - t_i) * (delta_i_mask \n",
    "                                          if t_i_cat is None \n",
    "                                          else delta_i_mask_cat)        # size: N x 1\n",
    "\n",
    "    neg_dlt_sig_i: Tensor = -delta_i * sigma_i  # [a, b, ..., 0]        # size: (N x 1) * (B x N x 1) -> B x N x 1\n",
    "    neg_dlt_sig_im1 = neg_dlt_sig_i.roll(1, -2) # [0, a, b, ...]        # size: B x N x 1\n",
    "    T_i = neg_dlt_sig_im1.cumsum(-2)            # [0, A, B, ...]        # size: B x N x 1\n",
    "\n",
    "    w = T_i * (1 - torch.exp(neg_dlt_sig_i))                            # size: (BxNx1) * (BxNx1) -> BxNx1\n",
    "    c = (w * c_i).sum(-2)                                               # size: ((BxNx1) * (BxNx3) -> BxNx3).sum(-2) -> Bx3\n",
    "    \n",
    "    return c, w, t_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat_cum_mask = torch.ones(B, config['Nc'], 1, requires_grad=False)\n",
    "w_hat_cum_mask[:,0] = 0\n",
    "\n",
    "def get_i_fine(\n",
    "    w: Tensor, \n",
    "    # b: int, \n",
    "    Nf: int, \n",
    "    i: Tensor\n",
    "):\n",
    "    w_hat = w / w.sum(-2, True)                                     # size: (BxNcx1) / ((BxNcx1).sum(-2, True) -> Bx1x1) -> BxNcx1\n",
    "    w_hat_cum = w_hat.cumsum(-2)                                    # size: BxNcx1\n",
    "    \n",
    "    u = torch.rand(B, 1, Nf)                                        # size: Bx1xNf\n",
    "    idx = (u > w_hat_cum).sum(-2) # 0≤u≤w0: [F,F,F,...].sum: idx=0  # size: ((Bx1xNf) > (BxNcx1) -> BxNcxNf).sum(-2) -> BxNf\n",
    "    \n",
    "    w_hat_cum = w_hat_cum.roll(1, -2) * w_hat_cum_mask\n",
    "    i_fine = (u - w_hat_cum) / w_hat + i # inv transf sample funcs  # size: ((Bx1xNf) - (BxNcx1) -> BxNcxNf) / (BxNcx1) + (Ncx1) -> BxNcxNf\n",
    "    \n",
    "    q = idx.flatten()                                               # size: BNf\n",
    "    p, r = torch.unravel_index(torch.arange(B * Nf), (B, Nf))       # size: BNf, BNf\n",
    "    i_fine = i_fine[p, q, r].view(B, Nf, 1) # pick correct function # size: BxNfx1\n",
    "    \n",
    "    return i_fine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader):\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # t: NDC coords for ray parameter\n",
    "    # Nc: num bins along ray at first\n",
    "    # Nf: num new samples based on Nc dist.\n",
    "    i = torch.arange(NC).unsqueeze(-1)                                              # size: Ncx1\n",
    "    for epoch in tqdm(range(num_epochs), desc='epochs'):\n",
    "        for o, d, c in tqdm(trainloader, desc='minibatches', leave=False):          # size: Bx1x3, Bx1x3, Bx3        \n",
    "            optimizer.zero_grad()   \n",
    "            b = o.shape[0] # might change in last batch \n",
    "\n",
    "            # get coarse colors \n",
    "            i_coarse = i + torch.rand(b, NC, 1)                                     # size: (Ncx1) + (BxNcx1) -> BxNcx1\n",
    "            c_coarse, w, t_i = color_weights_and_t_i(TN, TF, NC, \n",
    "                                                     i_coarse, \n",
    "                                                     o, d, model)                   # Bx3, BxNcx1\n",
    "\n",
    "            # get fine colors   \n",
    "            i_fine = get_i_fine(w, NF, i)                                           # size: BxNcx1\n",
    "            c_fine, _, _ = color_weights_and_t_i(TN, TF, NF, i_fine, \n",
    "                                                 o, d, model, t_i)                  # size: Bx3, BxNcx1\n",
    "            \n",
    "            loss: Tensor = criterion(c_coarse, c) + criterion(c_fine, c)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # not logging accuracy b/c continuous values \n",
    "            # may be close but will rarely equal each other\n",
    "            wandb.log({'loss': loss}) \n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ae7f8074344a7e981196190961f2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/8256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19c507ff61e46e88351790c8adbf755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "minibatches:   0%|          | 0/49612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(False)\n",
    "train(model, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Weights and Biases!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model.pth\"\n",
    "torch.save(model, model_path)\n",
    "wandb.save(model_path)\n",
    "print(\"Model saved to Weights and Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6769.81689</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">initial-run</strong> at: <a href='https://wandb.ai/jay-okoro/nerf/runs/autiswab' target=\"_blank\">https://wandb.ai/jay-okoro/nerf/runs/autiswab</a><br/> View project at: <a href='https://wandb.ai/jay-okoro/nerf' target=\"_blank\">https://wandb.ai/jay-okoro/nerf</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240427_190917-autiswab/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run stopped\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
